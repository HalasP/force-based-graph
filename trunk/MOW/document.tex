\documentclass{article}
\usepackage{graphicx}
\usepackage{polski}
\usepackage[cp1250]{inputenc}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{amssymb,amsmath}
\title{Metod Odkrywania Wiedzy\\Sprawozdanie z projektu}

\author{Wojciech Klicki\\Konrad Starzyk}
\begin{document}

\maketitle
\section{Zadanie}
Zadanie sk³ada siê z dwóch czêœci : implementacyjnej oraz badawczej. 
Czêœæ implementacyjna polega na implementacji metody dokonuj¹cej 
predykcji w œrodowisku GNU R. Na czêœæ drug¹ sk³ada siê szereg 
testów testuj¹cych skutecznoœæ algorytmu.
\newline
Realizacja zadania polega na wykonaniu poni¿szych etapów:
\begin{itemize}
  \item Postawienie pytañ dotycz¹cych danych wejœciowych oraz algorytmu
  kooperatywnej filtracji, na które odpowiedŸ powinniœmy uzyskaæ przy pomocy Slope-One.
  \item Analiza danych – wybranie kategorii oraz atrybutów, które bêd¹
  analizowane przez nasz algorytm.
  \item Implementacja algorytmu.
  \item Wykonanie eksperymentów, a nastêpnie ocena jakoœci klasyfikatora.
\end{itemize}

\section{Opis algorytmów}
\subsection{Kooperatywna filtracja}
Kooperatywna filtracja polega na przewidywaniu ocen jakie otrzymaj¹
 produkty od poszczególnych u¿ytkowników na podstawie ju¿ ocenionych
  produktów. Zgadniêcie oceny polega na za³o¿eniu pewnego podobieñstwa
   oceny jak¹ wystawi u¿ytkownik do ocen ju¿ wystawionych.
\subsection{Technika Slope-One}
Przewidywanie preferencji za pomoc¹ techniki Slope-One opiera siê na za³o¿eniu, ¿e
ocenê u¿ytkownika mo¿na przybli¿yæ za pomoc¹ wzoru \begin{math}{f(x)=x+b}\end{math}, który wyznacza 
œredni¹ ró¿nicê pomiêdzy ocenami dwóch u¿ytkowników którzy dokonali oceny 
tego samego elementu. Jest to oczywiœcie daleko id¹ce uproszczenie – w 
rozwiniêciu tej techniki mo¿na korzystaæ z predyktorów o wzorach \begin{math}{f(x)=ax+b}\end{math}
lub nawet \begin{math}{f(x)=ax^2+bx+c}\end{math}. Jak siê jednak okazuje, nawet
taki predyktor jest w stanie trafnie przewidywaæ preferencje u¿ytkowników. 
\newline
Oznaczmy przez \begin{math}v_i\end{math} i \begin{math}w_i\end{math} tablice
ocen dla dwóch ró¿nych u¿ytkowników, gdzie \begin{math}{i=1..n}\end{math} jest indeksem przedmiotu. Wtedy
\begin{math}{v_i-w_i}\end{math} jest ró¿nic¹ ocen tego samego przedmiotu przez dwóch u¿ytkowników. Spróbujmy znaleŸæ wartoœæ która
nalepiej przybli¿a ró¿nicê w ocenach dawanych przez tych u¿ytkowników.\newline
Minimalizuj¹c wyra¿enie:
\begin{math}
\sum_{i}{(v_i+b-w_i)^2}
\end{math}
ze wzglêdu na parametr \begin{math}b\end{math} otrzymujemy
\begin{math}b = \dfrac{\sum_{i}{v_i-w_i}}{n}\end{math}.
\newline \newline
Maj¹c zbiór testowy \begin{math}\kappa\end{math} oraz dowolne dwa oceniane
przedmioty \begin{math}i\end{math} oraz \begin{math}j\end{math}, wraz z ich
ocenami \begin{math}u_i\end{math} oraz \begin{math}u_j\end{math} mo¿emy
okreœliæ œrednie odchylenie przedmiotu \begin{math}i\end{math} wzglêdem
\begin{math}j\end{math} jako:
$$dev_{i,j} = \sum_{u \in S}{\dfrac{u_j-u_i}{|S|}} $$
gdzie jako \begin{math}S\end{math} oznaczymy zbiór ocen które
zawiera³y obydwa przedmioty.\newline
Bior¹c pod uwagê, ¿e nieznan¹ wartoœæ oceny przedmiotu
\begin{math}j\end{math} mo¿emy przewidywaæ jako \begin{math}u_j =
dev_{i,j}+u_i\end{math}, sensowny predyktor móg³by byæ œredni¹ takich
przewidywañ:
$$P(u)_j=\dfrac{1}{|R_j|}\sum_{i\in R_j}(dev_{i,j}+u_i)$$
gdzie \begin{math}R_j\end{math} jest zbiorem wszystkich przedmiotów \begin{math}
i\end{math} które zosta³y ocenione i dla których istnieje wyznaczona wartoœæ
œredniego odchylenia wzglêdem przedmiotu \begin{math}j\end{math} równa
\begin{math}dev_{i,j}\end{math}.

\section{Plan eksperymentów}
\subsection{Pytania}
\begin{itemize}
  \item Jaka bêdzie przewidywana ocena danego filmu przez danego
  u¿ytkownika wyznaczona przez algorytm?
  \item Jaka bêdzie trafnoœæ tej oceny?
\end{itemize}

\subsection{Charakterystyka zbiorów danych}
Dane u¿ywane do testów pochodz¹ z serwisu Movielens. Dostêpne s¹ dwa zestawy
danych: pierwszy sk³ada siê z 100,000 ocen 1682 filmów wystawionych przez 943
u¿ytkowników. Drugi zawiera oko³o miliona ocen 3900 filmów wystawionych przez
600 u¿ytkowników. Ka¿dy film zosta³ oceniony w skali od 1 do 5.

Do naszych badañ zostanie wykorzystany pierwszy zbiór zawieraj¹cy 100000
rekordów. Zosta³ on ju¿ wstêpnie podzielony na podzbory : trenuj¹cy (80000
ocen) oraz testowy (20000).

\subsection{Parametry algorytmów których wp³yw na wyniki bêdzie badany}

Algorytm nie zawiera parametrów którymi mo¿na sterowaæ jego dzia³aniem. 

\subsection{Sposób oceny jakoœci modeli}
Aby oceniæ trafnoœæ przewidywañ porównamy je z danymi ze zbioru testowego.
Ka¿d¹ wyznaczon¹ ocenê \begin{math}\hat{u_j}\end{math} porównujemy z ocen¹
testow¹ \begin{math}u_j\end{math}.Poniewa¿ istnieje 5 mo¿liwych ocen, b³¹d dla
jednej oceny mo¿emy okreœliæ jako:
$$e_j = \dfrac{|\hat{u_j}-u_j|}{5}$$
gdzie \begin{math}j=1..n\end{math}, a \begin{math}n\end{math} to liczba
wszystkich przewidywañ. Œredni b³¹d bêdzie wiêc równy:
$$\sum_j\dfrac{|\hat{u_j}-u_j|}{5n} $$
St¹d miarê jakoœci oceny mo¿emy wyraziæ jako:
$$q = 1 - \sum_j\dfrac{|\hat{u_j}-u_j|}{5n} $$